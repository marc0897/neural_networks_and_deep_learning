{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Improving the way neural networks learn</h1>\n",
    "\n",
    "In this chapter I explain a suite of techniques which can be used to improve on our vanilla implementation of backpropagation, and so improve the way our networks learn.\n",
    "\n",
    "When using the quadratic cost function our neural networks have difficulty learning when they're badly wrong--which can make learning very slow. We want a way to avoid such slowdowns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introducing the cross-entropy cost function</h2>\n",
    "We define the cross-entropy cost function for this neuron by\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "  C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right],\n",
    "\\tag{57}\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The cross-entropy is positive, and tends toward zero as the neuron gets better at computing the desired output, $y$, for all training inputs, $x$. And the larger the error, the faster the neuron will learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'E:\\Github\\neural_networks_and_deep_learning\\src')\n",
    "\n",
    "import network2\n",
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "\"\"\"\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import cPickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    f = gzip.open('data/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9046 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9044 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9355 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9333 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9333 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9371 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9422 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9439 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9469 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9478 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9474 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9496 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9502 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9487 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9513 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9496 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9497 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9474 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9490 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9499 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9517 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9500 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9506 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9493 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9533 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9430 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9514 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9520 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9508 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9505 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [9046,\n",
       "  9044,\n",
       "  9355,\n",
       "  9333,\n",
       "  9333,\n",
       "  9371,\n",
       "  9422,\n",
       "  9439,\n",
       "  9469,\n",
       "  9478,\n",
       "  9474,\n",
       "  9496,\n",
       "  9502,\n",
       "  9487,\n",
       "  9513,\n",
       "  9496,\n",
       "  9497,\n",
       "  9474,\n",
       "  9490,\n",
       "  9499,\n",
       "  9517,\n",
       "  9500,\n",
       "  9506,\n",
       "  9493,\n",
       "  9533,\n",
       "  9430,\n",
       "  9514,\n",
       "  9520,\n",
       "  9508,\n",
       "  9505],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "\n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Softmax</h2>\n",
    "The idea of softmax is to define a new type of output layer for our neural networks. We don't apply the sigmoid function to get the output. Instead, we apply the so-called softmax function to the output layer.\n",
    "\n",
    "According to this function, the activation $a^L_j$ of the $j^th$ output neuron is\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "  a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}},\n",
    "\\tag{78}\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where in the denominator we sum over all the output neurons.\n",
    "\n",
    "The softmax function makes the output layer akin to a probability distribution. All the outputs sum up to one and when we increase the inputs to one neuron, all the other neurons descrease. It creates a convenient way to interpret the output activations of the network.\n",
    "\n",
    "https://machinelearningmastery.com/softmax-activation-function-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Overfitting and regularization</h2>\n",
    "\n",
    "If we train too much, our network no longer generalizes to the test day. And so it's not useful learning. In this case, we say the network is **overfitting** or **overtraining**.\n",
    "\n",
    "Looking at this graph:\n",
    "\n",
    "![Overfitting](imgs/overfit_2.PNG)\n",
    "\n",
    "We might believe that our model is getting continuously better. But when we look at this figure:\n",
    "\n",
    "![Overfitting](imgs/overfit_1.PNG)\n",
    "\n",
    "The accuracy stops improving around epoch 280. This shows that our learning was merely an illusion.\n",
    "\n",
    "Usually, when the accuracy stops improving, we should stop training. This will help us avoid overfitting. We should use the validation data to measure accuracy between each epoch and stop when we don't improve accuracy to some degree. This method is called **early stopping**.\n",
    "\n",
    "The **validation data set** are examples that are held back to tune hyper parameters. We don't want to tune hyper parameters with the test data or training data because the parameters may just fit to the data. Using a validation data set reduces this bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Regularization</h3>\n",
    "\n",
    "We can reduce overfitting with **regularization**, specifically with **L2 regularization**. The idea is that we add an extra term to the cost function, a term called the regularization term. The regularized cross-entropy function would be:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} C = -\\frac{1}{n} \\sum_{xj} \\left[ y_j \\ln a^L_j+(1-y_j) \\ln\n",
    "(1-a^L_j)\\right] + \\frac{\\lambda}{2n} \\sum_w w^2.\n",
    "\\tag{85}\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We've added a second term, namely the sum of the squares of all the weights in the network. This is scaled by a factor $\\lambda/2n$, where $\\lambda>0$ is known as the **regularization parameter**, and n is, as usual, the size of our training set.\n",
    "\n",
    "Intuitively, the effect of regularization is to make it so the network prefers to learn small weights, all other things being equal. Large weights will only be allowed if they considerably improve the first part of the cost function.\n",
    "\n",
    "Looking at the same example (but with regularization) as last time, we see the cost changes the same:\n",
    "\n",
    "![Overfitting](imgs/overfit_3.PNG)\n",
    "\n",
    "But the accuracy increases over the entire 400 epochs. Clearly, we've avoided overfitting with regularization.\n",
    "\n",
    "![Overfitting](imgs/overfit_4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Other techniques for regularization</h2>\n",
    "\n",
    "<h3>Dropout</h3>\n",
    "We randomly (and temporarily) delete half the hidden nuerons in a network, while leaving the input and output neurons untouched.\n",
    "\n",
    "We train the selected hidden nuerons then restore the dropped neurons. We select a new subset of neurons for the next mini-batch and repeat the process.\n",
    "\n",
    "This is equivalent to training multiple networks then averaging their outputs. It avoids overfitting and turns out to be pretty accurate.\n",
    "\n",
    "<h3>Artificially expanding the training data</h3>\n",
    "Finding more data is a good way to improve a network, but it can be expensive to get more data. We can instead modify the data we have to make more data.\n",
    "\n",
    "For example, we can rotate the handwritten digits a couple degrees and add the modified digits to the data.\n",
    "\n",
    "<h3>An aside on big data and what it means to compare classification accuracies</h3>\n",
    "\n",
    "Suppose we have two algorithms A and B. Sometimes algorithm A will outperform algorithm B with one set of training data. But does that make A a better algorithm then B?\n",
    "\n",
    "Sometimes, when we change to data, algorithm B will outperform algorithm A. Usually, we look for both better algorithms and training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Weight Initialization</h2>\n",
    "\n",
    "As described in chapter 1, we have initialized weights and biases using independent Gaussian random variables, normalized to have mean 0 and standard deviation 1. The distribution of the weighted sum $z = \\sum_j w_j x_j + b$ will look like:\n",
    "\n",
    "![Distribution](imgs/distribution_2.PNG)\n",
    "\n",
    "But with this method, the sum is often very small or very large. This causes the output of the hidden neurons to be very close to 0 or 1, so the hidden neuron will have saturated. Thus, learning will be very slow.\n",
    "\n",
    "Choosing a different cost function helps descrease saturation in the output neurons, but it does nothing for saturated hidden neurons.\n",
    "\n",
    "Instead, we will initialized weights as Gaussian random variables with mean 0 and standard deviation $1/\\sqrt{n_{\\rm in}}$. The weighted sum $z = \\sum_j w_j x_j + b$ will be a Gaussian distribution with mean 0, but it'll be much more sharply peaked than before.\n",
    "\n",
    "![Distribution](imgs/distribution_1.PNG)\n",
    "\n",
    "The initialization procedure for biases doesn't really matter, wo we'll continue to use the old method.\n",
    "\n",
    "With the new method, we get a classification accuracy of,\n",
    "\n",
    "![Classification Accuracy](imgs/accuracy.PNG)\n",
    "\n",
    "Which ends up being the same accuracy but the network learns better. Sometimes, the weight initilization will result in higher classification accuracies.\n",
    "\n",
    "<h2>How to choose a neural network's hyper parameters</h2>\n",
    "\n",
    "<h3>Broad Strategy</h3>\n",
    "\n",
    "Our first goal should be to create a network that can achieve better results than chance. We can start by doing the following:\n",
    "\n",
    "- stripping a problem down to its base form--like distinguishing 1s and 0s instead of all ten digits. \n",
    "- stripping our network down to its simpliest form that can achieve meaningful results\n",
    "- increase the frequency of monitoring (say every 1000 images instead of 50,000\n",
    "\n",
    "We're going to get a lot of noise but at least we're getting feedback a lot faster. Now we can experiment with hyper parameters and when we get a singnal that something is working, we can implement it in the bigger problem\n",
    "\n",
    "<h3>Learning Rate</h3>\n",
    "\n",
    "1. Start by finding the order of magnitude that decreases that cost during the first few epochs.\n",
    "2. Find the largest value where the cost decreases during the first few epochs.\n",
    "3. Experiment with values lower than the max learning rate and of the same magnitude\n",
    "\n",
    "Learning rate won't affect the final classification accuracy so we do not have to use a validation data set.\n",
    "\n",
    "<h3>Use early stopping to determine the number of training epochs</h3>\n",
    "\n",
    "Early stopping is terminating training after classifcation accuracy stops improving by a certain amount after a certain number of epochs.\n",
    "\n",
    "<h3>Learning rate schedule</h3>\n",
    "\n",
    "We can start with a large learning rate and decrease it when validation accuracy starts to get worse. A variable learning schedule can improve performance but it creates many more hyper-parameters to tweak.\n",
    "\n",
    "<h2>Other models of artificial neuron</h2>\n",
    "\n",
    "We can use\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " \\tanh(w \\cdot x+b), \n",
    "\\tag{109}\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "instead of a sigmoid neuron. The values range from -1 to 1.\n",
    "\n",
    "We can also use\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\max(0, w \\cdot x+b).\n",
    "\\tag{112}\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "<h2>On stories of neural networks</h2>\n",
    "\n",
    "We don't always have rigourous mathematical proofs on whether certain techniques in neural networks work. Neural networks challenge the scope of human understanding and it will take decades before we can fully understand them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
