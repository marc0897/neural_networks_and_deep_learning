{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>How the backpropagation algorithm works</h1>\n",
    "\n",
    "In this chapter we'll explain a fast algorithm for computing such gradients, an algorithm known as backpropagation.\n",
    "\n",
    "At the heart of backpropagation is an expression for the partial derivative $\\frac{\\partial C}{\\partial w}$ of the cost function $C$ with respect to any weight $w$ (or bias $b$) in the network. The expression tells us how quickly the cost changes when we change the weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A fast matrix-based approach to computing the output from a neural network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's warm up with a fast matrix-based algorithm to compute the output from a neural network. We'll use $w^l_{jk}$ to denote the weight for the connection from the $k^{th}$ neuron in the $(l-1)^{\\rm th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer.\n",
    "\n",
    "![Figure 2.1](imgs/network1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use $b^l_j$ for the bias of the $j^{th}$ neuron in the $l^{th}$ layer. And we use $a^l_j$ for the activation of the $j^{th}$ neuron in the $l^{th}$ layer. For example,\n",
    "\n",
    "![Figure 2.2](imgs/network2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation $a^l_j$of the $j^{th}$ neuron in the $l^{th}$ layer is related to the activations in the $(l-1)^{\\rm th}$ layer by the equation\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "  a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right),\n",
    "\\tag{23}\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "over all neurons $k$ in the $(l-1)^{\\rm th}$ layer\n",
    "\n",
    "To rewrite this expression in a matrix form we define a weight matrix $w^l$ for each layer, $l$. The entries of the weight matrix $w^l$ are just the weights connecting to the $l^{th}$ layer of neurons, that is, the entry in the $j^{th}$ row and $k^{th}$ column is $w^l_{jk}$. Similarly, for each layer $l$ we define a bias vector, $b^l$. And finally, we define an activation vector $a^l$ whose components are the activations $a^l_j$.\n",
    "\n",
    "The last ingredient we need to rewrite (23) in a matrix form is the idea of vectorizing a function such as $\\sigma$. We want to apply a function such as Ïƒ to every element in a vector $v$. We use the obvious notation $\\sigma(v)$ to denote this kind of elementwise\n",
    "\n",
    "So, equation (23) can be written as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "  a^{l} = \\sigma(w^l a^{l-1}+b^l).\n",
    "\\tag{25}\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Before we apply the sigmoid function, we comput the intermediate quantity $z^l \\equiv w^l a^{l-1}+b^l$. We call this the *weighted input* to the neurons in layer $l$.\n",
    "\n",
    "Equation (25) is sometimes written as $a^l = \\sigma(z^l)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The two assumptions we need about the cost function</h2>\n",
    "\n",
    "The goal of backpropagation is to compute the partial derivatives $\\partial C / \\partial w$ and $\\partial C / \\partial b$ of the cost function $C$ with respect to any weight w or bias b in the network.\n",
    "\n",
    "*$\\partial C / \\partial w$ can be interpreted as the instantaneous change of $C$ with respect to the weights. That is, we want to find the value of weights that make C the smallest.*\n",
    "\n",
    "We need to make two main assumptions about the cost function for backpropagation to work. We will use the quadratic cost function as an example in the form:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  C = \\frac{1}{2n} \\sum_x \\|y(x)-a^L(x)\\|^2,\n",
    "\\tag{26}\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "* $n$ is the total number of training examples <br>\n",
    "* the sum is over individual training examples, $x$ <br>\n",
    "* $y=y(x)$ is the corresponding desired output <br>\n",
    "* $L$ denotes the number of layers <br>\n",
    "* $a^L = a^L(x)$ is the vector activations output from the network when x is input\n",
    "\n",
    "<h4>First Assumption</h4>\n",
    "\n",
    "**The cost function can be written as an average $C = \\frac{1}{n} \\sum_x C_x$ over cost functions $C_x$ for individual training examples, $x$.**\n",
    "\n",
    "For the quadratic cost function, a single training example is $C_x = \\frac{1}{2} \\|y-a^L \\|^2$\n",
    "\n",
    "Backpropagation computes the partial derivatives $\\frac{\\partial C_x}{\\partial w}$ and $\\frac{\\partial C_x}{\\partial b}$ at a single training example and recovering $\\frac{\\partial C}{\\partial w}$ and $\\frac{\\partial C} {\\partial b}$ by averaging over training examples.\n",
    "\n",
    "<h4>Second Assumption</h4>\n",
    "\n",
    "**The cost can be written as a function of the outputs from the neural network.**\n",
    "\n",
    "![Network Output](imgs/cost_function_network.PNG)\n",
    "\n",
    "Remember (in the case of the quadratic cost function) that a training example is fixed, so the output y is also fixed. So, the cost function is only a function of output activations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Hadamard product</h2>\n",
    "\n",
    "Suppose $s$ and $t$ are two vectors of the same dimension. Then we use $s \\odot t$ to denote the elementwise product of the two vectors.\n",
    "\n",
    "For example,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right] \n",
    "  \\odot \\left[\\begin{array}{c} 3 \\\\ 4\\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c} 1 * 3 \\\\ 2 * 4 \\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c} 3 \\\\ 8 \\end{array} \\right].\n",
    "\\tag{28}\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The four fundamental equations behind backpropagation</h2>\n",
    "\n",
    "The four fundamental equations of backpropagation calculate the error for the entire neural network. Also, we've learnt that a weight will learn slowly if either the input neuron is low-activation, or if the output neuron has saturated, i.e., is either high- or low-activation.\n",
    "\n",
    "![Equations](imgs/equations.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The backpropagation algorithm</h2>\n",
    "\n",
    "Backpropagation tells us how much we need to change the weights and biases of the network to get our desired output.\n",
    "\n",
    "Suppose we wanted to classify a handwritten digit as the number two.\n",
    "\n",
    "![Example](imgs/backprop_example.PNG)\n",
    "\n",
    "We need to increase the activation of the neuron associated with a two and decrease the activation of all other output neurons. But we can't directly change to activations. We can only adjust the weights and biases. However, we want to keep track of how much we want each activation neuron to change. Then, we can figure out how much we want the activations in the previous layer to change to get our desired output. Then every other layer has to be changed to adjust the second to last layer. **We will move backwards through the neural network and keep track of how much each layer needs to change to get the desired output of the next layer.**\n",
    "\n",
    "![Example 2](imgs/backprop_example2.PNG)\n",
    "\n",
    "The desired adjustments of all the neurons are added together to figure out how much we need to change the second to last layer. We then recursively apply the same process to every layer before the second to last layer. That is, we propagate backwards.\n",
    "\n",
    "We apply the backpropagation algorithm to every training example and find an average for how much we want to adjust each weight. The average amount we want to change each weight is the negative gradient of the cost function (multiplied by the learning rate).\n",
    "\n",
    "![Example_3](imgs/backprop_example3.PNG)\n",
    "\n",
    "Notes taken from: https://www.youtube.com/watch?v=Ilg3gGewQ5U&t=41s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
