{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>How the backpropagation algorithm works</h1>\n",
    "\n",
    "In this chapter we'll explain a fast algorithm for computing such gradients, an algorithm known as backpropagation.\n",
    "\n",
    "At the heart of backpropagation is an expression for the partial derivative $\\frac{\\partial C}{\\partial w}$ of the cost function $C$ with respect to any weight $w$ (or bias $b$) in the network. The expression tells us how quickly the cost changes when we change the weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A fast matrix-based approach to computing the output from a neural network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's warm up with a fast matrix-based algorithm to compute the output from a neural network. We'll use $w^l_{jk}$ to denote the weight for the connection from the $k^{th}$ neuron in the $(l-1)^{\\rm th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer.\n",
    "\n",
    "![Figure 2.1](imgs/network1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use $b^l_j$ for the bias of the $j^{th}$ neuron in the $l^{th}$ layer. And we use $a^l_j$ for the activation of the $j^{th}$ neuron in the $l^{th}$ layer. For example,\n",
    "\n",
    "![Figure 2.2](imgs/network2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation $a^l_j% of the $j^{th}% neuron in the $l^{th}$ layer is related to the activations in the $(l-1)^{\\rm th}$ layer by the equation\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "  a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right),\n",
    "\\tag{23}\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "over all neurons $k$ in the $(l-1)^{\\rm th}$ layer\n",
    "\n",
    "To rewrite this expression in a matrix form we define a weight matrix wl for each layer, $l$. The entries of the weight matrix $w^l$ are just the weights connecting to the l^{th} layer of neurons, that is, the entry in the j^{th} row and k^{th} column is $w^l_{jk}$. Similarly, for each layer $l we define a bias vector, bl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
