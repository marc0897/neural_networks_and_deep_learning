{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>A visual proof that neural nets can compute any function</h1>\n",
    "\n",
    "<h3>Universality Theorem</h3>\n",
    "\n",
    "No matter what the function, there is guaranteed to be a neural network so that for every possible input, $x$, the value $f(x)$ is output from the network. This hold even if we restrict out network to a single layer of hidden neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Two caveats</h2>\n",
    "\n",
    "First, We don't mean that a network can be used to exactly compute any function. Rather, we can get an approximation that is as good as we want.\n",
    "\n",
    "Second, the class of function which can be approximated in the way described are the continuous functions. It won't in general be possible to approximate a discontinous function but we might be able to get close enough.\n",
    "\n",
    "Summing up, a more precise statement of the universality theorem is that ***neural networks with a single hidden layer can be used to approximate any continuous function to any desired precision.***\n",
    "\n",
    "<h2>Universality with one input and one output</h2>\n",
    "\n",
    "Let's focus on a network with two hidden nuerons and a single input and output neuron.\n",
    "\n",
    "![Network](imgs/4_1.PNG)\n",
    "\n",
    "If we set the weight to 999 and the bias to -400, then the output of the top hidden nueron looks like a step function.\n",
    "\n",
    "![Network](imgs/4_2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step functions are easier to work with than sigmoid functions. The position of the step is proportional to $b$, and inversely proportional to $w$.\n",
    "\n",
    "We will describe the hidden neurons using just a single parameter, $s$, which is the step position, $s = -b/w$. \n",
    "\n",
    "![Parameter s](imgs/4_3.PNG)\n",
    "\n",
    "Suppose we set the input weights of the hidden neurons very large and set $s$ to some value between 0 and 1. We can then set the bias to $b = -ws$. If we then set the values of the output weights to some value $h$ and $-h$ then we get a bump function like this:\n",
    "\n",
    "![Bump Function](imgs/4_4.PNG)\n",
    "\n",
    "And if we add more neurons then we get more bumps in the graph.\n",
    "\n",
    "![More Bumps](imgs/4_5.PNG)\n",
    "\n",
    "If we add a bunch of bumps then we can get a very close approximation of any function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Many input variables</h2>\n",
    "\n",
    "Suppose we have two inputs and set one of the weights to 0. Then we get an output that looks like:\n",
    "\n",
    "![Multiple Inputs](imgs/4_6.PNG)\n",
    "\n",
    "If we set the top input weight to 100, we get a step function.\n",
    "\n",
    "![Multiple Inputs Step](imgs/4_8.PNG)\n",
    "\n",
    "We can move the step function location around by modifying the bias. The actual location of the step point is $s_x \\equiv -b / w_1$.\n",
    "\n",
    "![Another one](imgs/4_7.PNG)\n",
    "\n",
    "We can move the graph in the $y$ direction by setting the weight from $x$ to 0 and the weight from $y$ to some value.\n",
    "\n",
    "![Another one](imgs/4_10.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a bump function.\n",
    "\n",
    "![Multivariable Bump Function](imgs/4_13.PNG)\n",
    "\n",
    "If we vary the parameter h, we can make a tower function:\n",
    "\n",
    "![Multivariable Tower Function](imgs/4_11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use these tower to approximate any continuous function.\n",
    "\n",
    "![Multivariable Many Towers](imgs/4_12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extension beyond sigmoid neurons</h2>\n",
    "\n",
    "Any function that can be approximated to a step function will work in place of the sigmoid function. Any function $s(z)$ that satisfis the following can be used:\n",
    "\n",
    "- $s(z)$ is well defined as $z \\rightarrow -\\infty$ and $z \\rightarrow \\infty$\n",
    "- There are two limits that are different from one another (so that it's a step, not just a flat graph)\n",
    "\n",
    "<h2>Fixing up the step function</h2>\n",
    "\n",
    "We could scale down our function by a factor of $M$, comptue the vaulues and add up the approximations. This will increase our overall approximation by a factor of $M$.\n",
    "\n",
    "For example, we could have two different approximations to $\\sigma^{-1} \\circ f(x)/2$ and add them together to get an approximation of $\\sigma^{-1} \\circ f(x)$. This will increase our approximation by a factor of 2.\n",
    "\n",
    "<h2>Conclusion</h2>\n",
    "\n",
    "To sum up: universality tells us that neural networks can compute any function; and empirical evidence suggests that deep networks are the networks best adapted to learn the functions useful in solving many real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
